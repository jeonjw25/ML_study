{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\r\n",
    "\r\n",
    "## 회귀분석 \r\n",
    "\r\n",
    ": 지도학습에서 데이터 변수들 간에 함수관계를 파악하여 통계적으로 추론하는 기술  \r\n",
    "독립변수에 대한 종속변수값의 평균을 구하는것.  \r\n",
    "<br/> \r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139453226-b9f94762-1be5-447b-87f2-0246212ebff2.png)  \r\n",
    "\r\n",
    "h() : 평균을 구하는 함수. \"회귀 모델\"  \r\n",
    "Y : 종속변수\r\n",
    "x : 독립변수(입력데이터)  \r\n",
    "b : 중요도(가중치)  \r\n",
    "회귀분석을 한다 -> h() 함수가 무엇인지 찾는 과정.\r\n",
    "\r\n",
    "<br/> \r\n",
    "<br/> \r\n",
    "<br/>  \r\n",
    "\r\n",
    "## 선형회귀의 문제점 \r\n",
    "\r\n",
    "종속변수 Y 가 성공, 실패인 문제에 대해 예측 모델링을 한다고 가정.  \r\n",
    "성공 = 1, 실패 = 0  \r\n",
    "이를 Linear Regression으로 모델링 하면 Y의 범위가 맞지 않는 문제 발생.  \r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139454333-509c5f80-5e81-45dd-8ce2-9e58a770c195.png)  \r\n",
    "\r\n",
    "<br/>\r\n",
    "Y의 범위는 0~1 사이어야 하는데 연속형 확률변수를 모델링 하는 Linear Regression에서는 -∞ ~ ∞가 됨.\r\n",
    "<br/>\r\n",
    "<br/>\r\n",
    "\r\n",
    "따라서 logit 변환을 통해 Y의 범위를 -∞ ~ ∞ 가 되도록 만들어줌. -> Logistic regression  \r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139470129-56ceef13-a034-4963-8e4c-1847a3ea8b1e.png)  \r\n",
    "\r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139469126-0361005b-c316-4954-83b1-d00fa49f6459.png)  \r\n",
    "\r\n",
    "## 로짓이란?\r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139469725-55fb6281-c939-472e-bb81-2314f0d15a5c.png)  \r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139470204-83267717-71cf-4ca8-9ba2-585cea6db727.png)  \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "P 에 대한 수식이 시그모이드 함수와 같아짐.\r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139470383-3ab47bb6-7900-4e72-93a6-7337184780aa.png)  \r\n",
    "## 시그모이드 그래프  \r\n",
    "\r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139470529-36bd124c-ec70-44fb-9435-28bacf50bf49.png)  \r\n",
    "\r\n",
    "<br/>\r\n",
    "<br/>\r\n",
    "\r\n",
    "## 로지스틱 회귀모델  \r\n",
    "\r\n",
    "![image](https://user-images.githubusercontent.com/54730375/139470489-e258d2a5-fa19-4ede-bfd2-e813bd529a9d.png)  \r\n",
    "\r\n",
    "<br/>\r\n",
    "<br/>  \r\n",
    "\r\n",
    "# 코드  \r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "class LogisticRegression:\r\n",
    "    def __init__(self, learning_rate=0.01, threshold=0.01, max_iterations=1000, fit_intercept=True, verbose=False):\r\n",
    "        self._learning_rate = learning_rate  # 학습 계수\r\n",
    "        self._max_iterations = max_iterations  # 반복 횟수\r\n",
    "        self._threshold = threshold  # 학습 중단 계수\r\n",
    "        self._fit_intercept = fit_intercept  # 절편 사용 여부를 결정\r\n",
    "        self._verbose = verbose  # 중간 진행사항 출력 여부\r\n",
    "\r\n",
    "    # 가중치 return\r\n",
    "    def get_coeff(self):\r\n",
    "        return self._W\r\n",
    "\r\n",
    "    #  bias 추가\r\n",
    "    def add_intercept(self, x_data):\r\n",
    "        intercept = np.ones((x_data.shape[0], 1))\r\n",
    "        return np.concatenate((intercept, x_data), axis=1)\r\n",
    "\r\n",
    "    # 시그모이드 함수(로지스틱 함수)\r\n",
    "    def sigmoid(self, z):\r\n",
    "        return 1 / (1 + np.exp(-z))\r\n",
    "\r\n",
    "    def cost(self, h, y): # 자연로그가 들어가므로 일반적인 오차공식 사용x\r\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\r\n",
    "\r\n",
    "    def fit(self, x_data, y_data):\r\n",
    "        num_examples, num_features = np.shape(x_data)\r\n",
    "\r\n",
    "        if self._fit_intercept:\r\n",
    "            x_data = self.add_intercept(x_data)\r\n",
    "\r\n",
    "        # 가중치 설정\r\n",
    "        self._W = np.zeros(x_data.shape[1]) # feature 수만큼\r\n",
    "\r\n",
    "        for i in range(self._max_iterations):\r\n",
    "            z = np.dot(x_data, self._W)\r\n",
    "            hypothesis = self.sigmoid(z)\r\n",
    "\r\n",
    "            # 실제값과 예측값의 차이\r\n",
    "            diff = hypothesis - y_data\r\n",
    "\r\n",
    "            # cost 함수\r\n",
    "            cost = self.cost(hypothesis, y_data)\r\n",
    "\r\n",
    "            # cost 함수의 편미분 : transposed X * diff / n\r\n",
    "            gradient = np.dot(x_data.transpose(), diff) / num_examples\r\n",
    "\r\n",
    "            # gradient에 따라 theta 업데이트\r\n",
    "            self._W -= self._learning_rate * gradient\r\n",
    "\r\n",
    "            # 판정 임계값에 다다르면 학습 중단\r\n",
    "            if cost < self._threshold:\r\n",
    "                return False\r\n",
    "\r\n",
    "            # 100 iter 마다 cost 출력\r\n",
    "            if (self._verbose == True and i % 100 == 0):\r\n",
    "                print('cost :', cost)\r\n",
    "\r\n",
    "    def predict_prob(self, x_data):\r\n",
    "        if self._fit_intercept:\r\n",
    "            x_data = self.add_intercept(x_data)\r\n",
    "\r\n",
    "        return self.sigmoid(np.dot(x_data, self._W))\r\n",
    "\r\n",
    "    def predict(self, x_data):\r\n",
    "        # 0,1 에 대한 판정 임계값은 0.5 -> round 함수로 반올림\r\n",
    "        return self.predict_prob(x_data).round()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}